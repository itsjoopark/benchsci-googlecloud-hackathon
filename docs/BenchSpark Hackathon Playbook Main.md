# **Participant Playbook**

## BenchSpark Hackathon powered by Google Cloud

***Your go-to resource for everything you need to know about the BenchSpark Hackathon powered by Google Cloud \- use the document tabs or the table of contents to navigate\!***

## 

## **Table of Contents** {#table-of-contents}

[Table of Contents](#table-of-contents)

[Welcome & Overview](#welcome-&-overview)

[Guidelines & Essentials](#guidelines-&-essentials)

[Agenda, Location & Logistics](#agenda,-location-&-logistics)

[Teams and Setup](#teams-and-setup)

[The Challenge](#the-challenge)

[GCP Technical Set-up & Services Provided](#gcp-technical-set-up-&-services-provided)

[Technical Support & Mentorship](#technical-support-&-mentorship)

[Learning Resources & Documentation](#learning-resources-&-documentation)

[Project Submissions](#project-submissions)

[Judging & Evaluation](#judging-&-evaluation)

[Help & Troubleshooting](#help-&-troubleshooting)

[Code of Conduct](#code-of-conduct)

[Closing Notes](#closing-notes)

### 

## **Welcome & Overview**  {#welcome-&-overview}

Welcome to the BenchSpark Hackathon\! This handbook provides you with essential guidelines, resources, and tips to make the most of your hackathon experience. **Please read carefully before the event.**

### **Event Goals**

* *Explore the fundamentals of building with AI, and AI systems*  
* *Get experience working with Google Cloud Services*  
* *Connect with experts, mentors, and peers who share your passion for AI innovation.*  
* *Showcase your work in front of BenchSci and Google Cloud team members*

### **The Challenge**

*Create an MVP AI co-scientist that helps preclinical scientists explore public biomedical data, generate hypotheses, and execute focused tasks (e.g., retrieval, analysis, evaluation).*

More details [here](#the-challenge).

### 

### **Hackathon Format Overview**

| When: | 2 days Feb. 26-27, 2026 8:30am-8pm on Feb. 26 8:30am-5pm on Feb. 27 *Participants will have facility access during these times. Work may continue outside of these hours if participants choose to.* |
| :---- | :---- |
| **Where:** | **In person** at the BenchSci offices in Toronto 559 College Street Suite 201 |
| **Teams:** | 2-5 eligible participants per team, you may join one team only.  *All team members must be registered in order to participate* |
| **Allowed Work:**  | Original work created by you and your team during the hackathon. Pre-existing materials that you own or are licensed to use (including open-source). You must follow all license terms and include any required attributions or notices for pre-existing materials. Do not include content you don’t have rights to, or anything that violates privacy, confidentiality, or the law. |

## **Guidelines & Essentials**  {#guidelines-&-essentials}

### **What to bring**

* Laptop and charger  
* Phone charge (if required)  
* Any other peripherals (e.g. mouse, keyboard etc) if you prefer to use them  
* Adapter for connecting to USB-C (to use external monitors, if required)  
* Headphones with a mic (for recording video walkthroughs, if required)  
* Government-issued ID (required for venue entry)  
* Comfortable clothing

### **Key Dates**

* **Feb. 22 at 11:59pm EST**  \- Deadline for submitting team set-up requirements (details [here](#teams-and-setup))  
* **Feb. 26 at 9am EST** \- Hackathon starts  
* **Feb. 27 at 1:30pm EST** \- Project submission deadline  
* **Mar. 6 at 9pm EST** \- Ephemeral GCP environment expires; make sure to save any materials you want to keep before this date

### **Important to Know**

* **Eligibility:** All participants must be 18 years or older and residents of Ontario. Employees, contractors, interns, officers, or directors of BenchSci or Google (and their household members) are not eligible to participate.  
* **Communications:** Join our hackathon Discord server to connect with other participants, form teams and stay up to date. Announcements and information leading up to the event will be shared here.  
* **Mentors & Support:** Mentors and experts from BenchSci and Google Cloud will be available throughout the event to offer support  
* **Food & Drinks:** Food, drinks and snacks will be provided. All participants should have submitted dietary restrictions as part of their registration.   
* **Intellectual Property (IP)** (per the full [terms & conditions](https://drive.google.com/file/d/1sBqhAsMX2d7xM5sbirQG_6cN7BbSj6ZS/view?usp=sharing))  
  * **You Keep Your IP:** You and your team own the intellectual property you create.  
  * **License to BenchSci:** In return for participating, you grant BenchSci a worldwide, perpetual, irrevocable, non-exclusive, transferable, sublicensable, royalty-free license to use, reproduce, adapt, modify, analyze, and create derivative works from your submission for any purpose related to BenchSci’s business (e.g., research, development, and promotion).  
  * **Moral Rights:** To the maximum extent allowed by law, you irrevocably waive your moral rights (like the right to attribution) in favor of BenchSci.  
* **Cloud Environment:** Eligible participants will receive access to an **ephemeral Google Cloud environment**, and a variety of tools within that environment  
  * Access starts: **Feb. 26 at 9am EST**  
  * Access **ENDS** and resources may be **DELETED** at: **Mar. 6 at 9pm EST.** It is your responsibility to export and save all code, data, notes, and models **before** the expiry time. BenchSci and Google are not responsible for anything lost after expiry.

Full details about [GCP technical set-up](#gcp-technical-set-up-&-services-provided)

## **Agenda, Location & Logistics** {#agenda,-location-&-logistics}

### **Schedule**

### **Day 1 \- Feb. 26, 2026**

* **8:30 AM:** Doors Open, Registration & Breakfast  
* **9:00 AM:** Opening Remarks & Kick-off  
* **9:30 AM:** Hacking Begins\!  
* **12:00 PM:** Lunch  
* **6:00 PM:** Dinner  
* **8:00 PM:** Venue Closes (End of Day 1\) \- teams may continue hacking

### **Day 2 \- Feb. 27, 2026**

* **8:30 AM:** Venue Opens & Breakfast  
* **12:00 PM:** Lunch  
* **1:30 PM:** [**SUBMISSIONS DUE**](https://drive.google.com/open?id=1G8EOQKn4ioQ9ex3xJMI5ODePceWcOPXAf6FsbzigNNQ)**,** Round 1 judging begins  
* **2:00 PM:** Fireside Chat: Working at BenchSci (optional) \- Join BenchSci team members and leaders for a candid fireside chat about the reality of working in tech today: how AI is changing our daily decision-making, the tension between innovation and delivery, and what it’s actually like to tackle "moonshot" ideas.  
* **3:00 PM:** Finalist Demos (Top 4 teams)  
* **4:00 PM:** Awards & Closing Remarks  
* **5:00 PM:** Event Ends

### **Venue Information**

* **Location:** BenchSci Office  
* **Address:** 559 College Street, Suite 201Toronto, ON  
* **Building Hours**: Open 8:30am \- 6:00pm  
* **WiFi Access:** Network and password will be provided at check-in.  
* **Parking:** Public Parking is available near the office:  
  * [Palmerston Ave](https://goo.gl/maps/VbyySxtXC6JubDDcA) Open Mon-Fri, 6 am-6 pm, Price: $4 each 1/2h or less, $14 daily maximum.  
  * [474 Bathurst Street](https://goo.gl/maps/mkGwLn79WrLVYRmu6), Open 24/7, Price: $4 each 1/2h or less, $15 Mon-Fri daily maximum.  
  * [74 Clinton Street](https://goo.gl/maps/zmUaU6pistjZPWtU9), Open 24/7, Price: $1.75 each 1/2h or less, $9 daily maximum.  
* **Venue Rules:**  
  * All participants must bring a government-issued ID to enter the venue.  
  * Respect the space and equipment.  
  * Follow all venue rules and safety instructions provided by staff

### **Event Video & Photographs**

* **Event Recording:** The event may be photographed, filmed, and recorded by BenchSci.  
* **Your Consent:** By participating, you consent to BenchSci’s use of your **name, likeness, voice, institution/affiliation, city/province, and submission** in promotional materials in any media, without additional payment.  
* **Opt-Out Option:** If you prefer not to be photographed or recorded, please inform the event staff at check-in. We will provide a visible identifier (e.g., a specific lanyard or sticker) and ask our media teams to avoid filming/photographing you in close-ups. This may not always be possible in wide crowd shots.

## **Teams and Setup** {#teams-and-setup}

Participants may choose to participate in teams of up to 5\. 

**If you are looking for team members,** head over to the BenchSpark Discord to connect with other participants to form a team.

To ensure that each team has a GCP project with necessary services set up in advance of the hackathon, **each team will be asked to submit the following information no later than Feb. 22 at 11:59pm EST using [this form](https://forms.gle/cvK5p8nVjWsjcfyR9)**.

* Team name  
* List of team members \- provide emails associated with Google accounts to be given access to the project  
* Specific problem you plan to work on (teams may select a specific use case from the list provided or provide their own)  
* If you require any additional GCP services or APIs beyond what is provided by default (see list below)  
* Your high level approach

Teams will have access to add additional data sources to their bucket before the hackathon. 

*Requirements or team members submitted after the deadline can still be accommodated, but we can’t guarantee it will be completed prior to the hackathon. Google Cloud coaches will also be able to help you add services and APIs during the hackathon as needed.*

## 

## **The Challenge** {#the-challenge}

***Create an MVP AI co-scientist that helps preclinical scientists explore public biomedical data, generate hypotheses, and execute focused tasks (e.g., retrieval, analysis, evaluation).***

Teams may opt to work on one of the following specific challenge areas defined by the BenchSci team, or they may choose to come up with their own idea related to creating an MVP AI co-scientist. 

*You may also use the tabs in this document to navigate to more details about each challenge.*

| 1️⃣ ChronosBio: Unraveling Biological Timelines from Static Snapshots | Biological processes happen in sequence, but experimental data gives us only snapshots. Your challenge: develop methods to infer the temporal order of events in biological pathways from static data. [More details](?tab=t.jfg4gl7de0jz) |
| :---- | :---- |
| 2️⃣ **Target or Trap?** Predict whether modulating a biological target will safely and effectively treat disease before costly development begins | Build a target assessment tool that predicts whether a biological target is likely to succeed or fail in clinical trials by integrating human genetics and expression data to evaluate causal disease involvement, directionality, and safety risk. [More details](?tab=t.uz51yibjgq6z) |
| 3️⃣ **Cracking the VUS Code:** Build an AI system to interpret uncertain genetic variants and predict clinical pathogenicity | Build an AI-powered system that interprets Variants of Unknown Significance (VUS) by combining population genetics, clinical evidence, and functional predictions to classify variants as likely pathogenic or benign. [More details](?tab=t.i3fklu8wvsd) |
| **4️⃣ Biomarkers that Matter:** Mine RNA, protein, and single-cell data to identify tissue-specific markers that guide treatment and improve trial success | Over 90% of oncology clinical trials now require companion biomarkers, yet finding reliable tissue-specific biomarkers remains a major bottleneck in drug development. Poor patient stratification leads to failed trials costing billions and delays potentially life-saving treatments. Your mission: Mine RNA, protein, and single-cell data to identify tissue-specific biomarkers that could guide treatment decisions and improve clinical trial success rates. [More details](?tab=t.5qcai1sobn6n) |
| **5️⃣ From Wandering to Wisdom:** Create a rich navigation experience for multi-hop scientific discovery that captures how humans actually explore complex knowledge | Build an interactive graph exploration system that lets scientists manually traverse multi-hop connections in biomedical knowledge graphs to discover, visualize, and validate complex relationships between biological entities (genes, diseases, drugs, pathways, proteins). [More details](?tab=t.9cr0ai7agw7) |
| **6️⃣ The Symmetric Bridge:** Closing the Semantic Gap in Drug Discovery RAG  | Build a Retrieval-Augmented Generation (RAG) system that overcomes the "semantic gap" in biomedical search. When a researcher asks "How does metformin affect autophagy?", your system should find relevant passages even when the documents use different terminology. [More details](?tab=t.dgxcwdspfxj4) |
| **7️⃣ Co-Investigator** — From Search Bar to Research Partner Challenge Statement | Build an agentic AI Research Assistant that operates like a high-level research intern—decomposing complex research requests into multi-step, event-driven workflows, tracking task state, and interacting with users to confirm next steps before proceeding. It identifies key researchers in a field, determines their current activity level, finds contact information, and synthesizes disease data with external sources to create comprehensive reports. The agent should be interactive, pausing to ask, "I’ve found this data—would you like me to research the lead scientists next?" [More details](?tab=t.uq6pll7r47ml) |
| **8️⃣ CLaRA Challenge:** Efficient Reasoning at Scale  | Build a next-generation scientific discovery engine that leverages CLaRA (Compressed Language-model Representations for Acceleration) to overcome the "bottleneck" of traditional Retrieval-Augmented Generation. [More details](?tab=t.a7a18lxmfw6x) |

## **GCP Technical Set-up & Services Provided** {#gcp-technical-set-up-&-services-provided}

### **Data Access Pattern**

* Each team will be assigned one dedicated GCP Project with a preset budget, in which to test, build and deploy their hackathon project.   
* Write access to this project starts at the start of the hackathon on Feb. 26 at 9am EST  
* A bucket with upload access will be made available the week prior. Please note that only publicly available data may be uploaded here. Do not include closed access data or Personally Identifiable Information (PII).   
* Participants will have access to a **master project** with shared, read-only [datasets](?tab=t.abs5kp9tz7fj).   
* **BigQuery is the preferred OLAP/data warehouse** for structured data, but participants may use any GCP native services, Google first-party services with per second billing and open-source tooling.  
* G**PU access will be limited to one GPU, L4 type,** If you anticipate needing additional GPUs, please indicate so on your team set-up requirements or reach out to the Google Coaches.   
* **Team projects are hosted on an ephemeral Google Cloud environment**. Access **ENDS** and resources may be **DELETED** at: **Mar. 6 at 9pm EST.** It is your responsibility to export and save all code, data, notes, and models **before** the expiry time. BenchSci and Google are not responsible for anything lost after expiry.

### **Provided Tooling, Services and APIs**

Available services will be limited to GCP native services, Google first-party services and open-source tooling, which can be run on the Google Cloud environment, or as a managed service within the project. Third-party commercial services are not allowed.

| Services provided by default |  |
| :---- | :---- |
| [BigQuery](https://cloud.google.com/bigquery?utm_source=google&utm_medium=cpc&utm_campaign=Cloud-SS-DR-GCP-1713658-GCP-DR-NA-CA-en-Google-BKWS-BRO-BigQuery&utm_content=c-Hybrid+%7C+BKWS+-+BRO+%7C+Txt-Data+Analytics-Data+Analytics-BigQuery-33969409261&utm_term=bigquery&gclsrc=aw.ds&gad_source=1&gad_campaignid=22976548691&gclid=CjwKCAiAkbbMBhB2EiwANbxtbXI8Ye2qbEF_KRRVuUmvkHfOBu58bY0fUVfHNEPTdIJ9Ex5vH_99bhoCdKwQAvD_BwE) | Serverless data warehouse for running fast SQL queries on massive datasets, perfect for analyzing large amounts of structured data without managing infrastructure. |
| [Vertex AI](https://cloud.google.com/vertex-ai) | Unified machine learning platform for building, training, and deploying AI models with pre-trained APIs for vision, language, and custom ML workflows, including [Vertex AI Workbench / Notebooks](https://cloud.google.com/vertex-ai/docs/workbench/introduction): A Jupyter notebook-based development environment for the entire data science workflow, ideal for Python-first scientists and engineers to experiment, prototype, and build demos. [Vertex AI Model Garden & Pre-trained Model](https://cloud.google.com/model-garden)s: A single place to discover, test, and deploy foundation models (text, code, multimodal) and embedding models for vector search. [Vertex AI Prediction / Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/overview): Managed service to deploy custom or pre-trained models for online prediction with scalable endpoints.  [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction): A serverless orchestration service for teams wanting simple MLOps or repeatable machine learning workflows.  [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview): A high-performance vector database (formerly Matching Engine) for RAG and semantic search use cases.  [Vertex AI Search & Conversation](https://cloud.google.com/enterprise-search): A managed platform (part of Agent Builder) to quickly build search and chat interfaces over your own data.  [Vertex AI Batch Prediction](https://cloud.google.com/vertex-ai/docs/predictions/overview#batch): An optional service for running large offline scoring jobs on bulk data without managing underlying infrastructure.  [Vertex AI Studio / Console Access](https://cloud.google.com/generative-ai-studio) A UI-driven environment that allows less-experienced users to test generative models via a web interface before exporting them to code. https://cloud.google.com/generative-ai-studio |
| [Cloud Spanner](https://cloud.google.com/spanner) | Globally distributed, horizontally scalable relational database that combines SQL capabilities with automatic sharding, ideal for applications requiring global consistency and high availability. |
| [Firebase](https://firebase.google.com/docs) | Backend-as-a-service platform providing real-time databases, authentication, hosting, and cloud functions to quickly build and deploy mobile/web apps without server management. |
| [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine?utm_source=google&utm_medium=cpc&utm_campaign=Cloud-SS-DR-GCP-1713658-GCP-DR-NA-CA-en-Google-BKWS-BRO-kubernetes-engine&utm_content=c-Hybrid+%7C+BKWS+-+BRO+%7C+Txt-AppMod-GKE-Kubernetes+Engine-336266607510&utm_term=google%20kubernetes%20engine&gclsrc=aw.ds&gad_source=1&gad_campaignid=22976548925&gclid=Cj0KCQiA7rDMBhCjARIsAGDBuEAwRlYfnhxCGL7-kvk9LBQaAOOBwZw2utpBhMP-hkOsneTS_q6GtVsaAnWeEALw_wcB)  | Google Kubernetes Engine is a managed environment for deploying, managing, and scaling containerized applications using Kubernetes. |
| [Cloud SQL](https://cloud.google.com/sql/mysql) | A fully managed relational database service for MySQL, PostgreSQL, and SQL Server.  |
| [Dataflow](https://cloud.google.com/products/dataflow) | A fully managed service for unified stream and batch data processing. |
| [Cloudrun](https://cloud.google.com/run) | A managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. |

### **Available Models & Model Training Limits**

* Participants will have access to the following models through Vertex AI. Note that whenever using a model, it is recommended to use its API endpoint instead of deploying the model on TPU/GPU accelerator to save costs. This is particularly important for Partner models.  
  * [**Google Models**](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models)  
  * [**Partner Models**](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models)  
* **Heavy GPU/TPU intensive model training is not allowed** as part of the Hackathon, as we want participants to focus on building their prototypes.   
* **Limited post‑training adaptation is allowed** as needed:   
  * SFT / RL-style tuning within reasonable resource usage

## **Technical Support & Mentorship** {#technical-support-&-mentorship}

* **Google Cloud support will be available** to all participants in-person during the Hackathon on Feb. 26 from 9:00 am to 5 pm EST and on Feb. 27 from 9:00am to 1:00pm EST  
* **BenchSci mentors with experience in science and engineering will be available throughout the hackathon** (exact hours/expertise will be shared closer to the hackathon)

## **Learning Resources & Documentation** {#learning-resources-&-documentation}

### **Unfamiliar with Google Cloud? Here are some resources to get your started**

Check out these learning modules on Google Skills. These are optional, but available to all hackathon participants. If you plan to use a service that you are unfamiliar with, we recommend reviewing one or two of these in advance of the Hackathon so that you’re ready to make the most of your time. 

These courses are optional and you do not need to do all of them \- select the ones that are

**You should have received an invitation, sent to the email address associated with your Google account that you provided at registration, which will give you access to 35 credits to use with Google Skills.** You will have access to these resources until Mar. 1, 2026\.

We’ve compiled a list of suggested courses, which you can [browse here](?tab=t.2m5n60asenv1), or check out the [learning path](https://www.skills.google/paths/3646) in the Google Skills platform. 

## **Project Submissions** {#project-submissions}

### **Requirements**

Each team must submit their hackathon project by the final deadline to be considered for the prize. All projects must be submitted for judging via the submission form ([submit here](https://docs.google.com/forms/d/e/1FAIpQLSfSvbyNIJeUB6cj5faBoZZH9cGUFnfJDvxyxQ_5DMxs1bjPpQ/viewform?usp=dialog)).

**Submission Deadline:** Feb. 27 at 1:30pm EST

**Submission Materials:**

* Demo video (3 minutes max), posted as an unlisted video on YouTube ([instructions](?tab=t.oq8pmh4kt0r))  
  * A working demo prototype (provide a link)  
  * A short project description (problem statement, solution, tools used)  
  * A link to your code repository (provide a link)  
  * A short readme, and any license attributions (link to a google doc, or upload a text file)

\*\*\*Late, incomplete, or corrupted submissions may be disqualified\*\*\* (reach out to [Chelsea Omel](mailto:comel@benchsci.com) immediately if you have issues with your submission)

### **Judging Process**

We will use a "Science Fair" format followed by finalist demos on stage for the top 4 teams: 3 judge-selected, one crowd favourite. In the event of a tie, it will be broken by: (1) Innovation/Novelty of the approach (2) Technical execution, based on a functional demo, scientific accuracy and data quality (3) Judges’ majority vote.

### **Finals Presentations**

Teams who are selected to advance to the finals will have the option to either present their submitted video demo, **or** present live on stage. Teams will be allocated 3 minutes for the presentation (recorded video or live) followed by 5 minutes of live Q\&A with the finalist judges.  

## **Judging & Evaluation**  {#judging-&-evaluation}

### **Judging Criteria**

A panel of BenchSci and Google judges will evaluate submissions based on the following rubric

| Criteria | Description | Weighting |
| ----- | ----- | ----- |
| **1\. Solution impact & relevance**  Problem significance (in the context of drug discovery) Problem clarity   | Is the biomedical problem clearly defined and important?  Is the underlying scientific assumption testable and non-trivial? (for example, does inferring temporal order from static data actually make sense for this pathway?) Would someone actually use this? Is the target user identifiable and the benefit clear? | **30 points** |
|  |  |  |
| **2\. Technical Execution & Novelty**  Functional demo Scientific accuracy & data quality Innovation / novelty of approach | Does it work end-to-end? Does the demo show it running with real data, not just slides? Is the code functional and distinct from a standard "Hello World" example? Are outputs scientifically sound? Did they validate against ground truth or domain knowledge? Do they handle data quality issues (missing values, known biases in sources like ClinVar or gnomAD)? Would a biologist trust the results? How are they choosing to use data to inform LLMs & hallucination checks?  Do they use a creative combination of tools, data, or methods that goes beyond following a tutorial? | **30 points** |
|  |  |  |
|  |  |  |
|  |  |  |
| **3\. Use of Google Cloud** Core AI Workflow Data Mastery Architecture Cost & Efficiency | How did they design and implement AI workflows?  Can they articulate why it was designed that way? How was the data used?  Were any additional datasets appropriate?   How did they design the architecture?  Did they put thought into what services were used and why?  Were GCP services used where appropriate?  Or were self-hosted solutions used instead? Is the solution efficient and cost-effective? | **20 points** |
|  |  |  |
|  |  |  |
| **4\. Presentation & Demo**  Presentation clarity Storytelling  | Did they sell the value proposition?  Did they speak the language of the user (the scientist) correctly? Can a non-specialist follow the narrative from problem → approach → result? Does the demo flow smoothly in the presentation? **For consideration in finals scoring:** Can they answer technical and scientific questions? Do they acknowledge limitations honestly? | **20 points** |
| **Total Score out of 100** |  |  |

### 

### **Prizes**

* **Winning Team:** One (1) winning team will be selected.  
* **Prize:** Non-cash prize(s) with an approximate retail value of **up to CAD $7,500 for the team**, and lunch with members of the BenchSci and Google team, hosted at the Google offices in Toronto on a mutually agreed date.   
* **Details:** No cash alternative. Prizes are non-transferable. Taxes, if any, are the responsibility of the recipients. 

## **Help & Troubleshooting** {#help-&-troubleshooting}

* **Before the hackathon:** Reach out by email, or post in the \#questions-for-organizers channel in our Discord server  
* **During the hackathon:** Mentors will be circulating during the event. You can also visit the designated help desk, or reach out via *BenchSpark Discord* in the \#questions-for-organizers channel  
* **Contact Information:**  
  * **Primary Contact:** Chelsea Omel  
  * **Email:** hello@benchsci.com 

## **Code of Conduct** {#code-of-conduct}

We are committed to a respectful, professional, and inclusive environment.

* Be respectful and professional. Harassment or discrimination will not be tolerated.  
* Do not build or present anything that is unsafe, unlawful, or unethical (e.g., malware, privacy violations, security bypasses).  
* Follow all venue rules and safety instructions.  
* Work with honesty and integrity.  
* Report any concerns to an organizer immediately.

BenchSci may disqualify anyone who breaks these rules or the event Terms & Conditions.

## **Closing Notes** {#closing-notes}

Take advantage of this unique opportunity to learn from BenchSci and Google Cloud experts, collaborate with peers, and build something amazing. The hackathon is about learning, experimenting, and pushing boundaries—so have fun, try bold ideas, and support your fellow builders.

Good luck, and happy hacking\!  
